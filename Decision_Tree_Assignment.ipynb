{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Decision Tree Assignment**"
      ],
      "metadata": {
        "id": "Mx3FhQQ0W-K-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Theoretical Questions**\n",
        "\n",
        "1. What is a Decision Tree, and how does it work ?\n",
        "  - A decision tree is a supervised learning algorithm used for both classification and regression tasks.\n",
        "  - It visually represents a series of decisions, starting from a root node and branching out to leaf nodes that represent the final outcome or prediction.\n",
        "\n",
        "2. What are impurity measures in Decision Trees ?\n",
        "  - Impurity measures in decision trees quantify the homogeneity or heterogeneity of a node's data.\n",
        "  - In simpler terms, they assess how mixed or unmixed the classes are within a node.\n",
        "\n",
        "3. What is the mathematical formula for Gini Impurity\t?\n",
        "  - The mathematical formula for Gini Impurity is 1 - Σ(pᵢ²), where pᵢ is the probability of a data point belonging to class i.\n",
        "  - This formula is used in decision tree algorithms to determine the best way to split data at each node, aiming to minimize impurity and maximize homogeneity in the resulting child nodes.\n",
        "\n",
        "4. What is the mathematical formula for Entropy\t?\n",
        "  - To calculate the entropy change for a phase transition, we can use the formula ΔS=ΔH/T, where ΔH is the enthalpy of fusion and T is the temperature. Given: ΔH = 6.01 kJ/mol and T = 273 K (0°C).\n",
        "\n",
        "5.  What is Information Gain, and how is it used in Decision Trees\t?\n",
        "  - Information Gain is a metric used in decision tree algorithms to determine the most relevant features for splitting data.\n",
        "\n",
        "6. What is the difference between Gini Impurity and Entropy\t?\n",
        "  - Gini impurity is generally faster to compute and often produces similar results to entropy, but entropy can be more sensitive to changes in class distribution, particularly in cases of imbalanced datasets.\n",
        "\n",
        "7. What is the mathematical explanation behind Decision Trees\t?\n",
        "  - Decision trees are built using mathematical concepts to recursively partition data based on feature values, aiming to create increasingly pure subsets.\n",
        "\n",
        "8. What is Pre-Pruning in Decision Trees ?\n",
        "  - Pre-pruning, also known as early stopping, is a technique used in decision tree algorithms to prevent overfitting by halting the tree's growth before it reaches its full potential.\n",
        "\n",
        "9. What is Post-Pruning in Decision Trees\t?\n",
        "  - Post-pruning, also known as backward pruning, is a technique used in decision tree algorithms to simplify the tree structure after it has been fully grown according to GitHub and potentially overfit the training data.\n",
        "  - It involves removing branches or subtrees from the fully grown tree, usually based on performance metrics on a validation set, to improve generalization to unseen data.\n",
        "\n",
        "10. What is the difference between Pre-Pruning and Post-Pruning\t?\n",
        "  - Pre-pruning and post-pruning are two strategies used to optimize decision trees by reducing their complexity and preventing overfitting.\n",
        "  - Pre-pruning stops the tree's growth during construction, while post-pruning simplifies a fully grown tree.\n",
        "\n",
        "11. What is a Decision Tree Regressor\t?\n",
        "  - A decision tree regressor is a machine learning model that uses a tree-like structure to predict continuous numerical values.\n",
        "\n",
        "12.  What are the advantages and disadvantages of Decision Trees ?\n",
        "  - Decision trees offer both advantages and disadvantages in machine learning.\n",
        "  - Advantages include ease of understanding and interpretation, ability to handle both categorical and numerical data, and less need for data preparation.\n",
        "\n",
        "13. How does a Decision Tree handle missing values ?\n",
        "  - Decision trees can handle missing values in several ways. One common approach is to use surrogate splits, where alternative features are used to guide data points with missing values down the appropriate branch.\n",
        "\n",
        "14. How does a Decision Tree handle categorical features ?\n",
        "  - Decision trees can naturally handle categorical features. For classification problems, the tree splits are determined based on the unique values of the categorical feature, effectively partitioning the data based on those categories.\n",
        "\n",
        "15. What are some real-world applications of Decision Trees ?\n",
        "  - Decision trees are versatile tools with applications across many fields, including business, healthcare, finance, and marketing. They are used for everything from predicting customer behavior to diagnosing diseases and assessing financial risk."
      ],
      "metadata": {
        "id": "BAJGJVIyXCzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Practical Questions**"
      ],
      "metadata": {
        "id": "IgytRtWyZEL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#16 Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Decision Tree Classifier Accuracy on Iris dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "pdufEbbSZH5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17 Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Decision Tree Classifier with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "id": "Nm2aZA6kZP4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18 Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Decision Tree Classifier using Entropy\n",
        "clf = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy (using entropy): {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "crCza_W0ZZM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE).\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "LReusGBMZiu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20 Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "import graphviz\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export the tree to Graphviz DOT format\n",
        "dot_data = export_graphviz(\n",
        "    clf,\n",
        "    out_file=None,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    special_characters=True\n",
        ")\n",
        "\n",
        "# Visualize the tree using graphviz\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"iris_tree\")  # Saves to iris_tree.pdf\n",
        "graph.view()  # Opens the rendered PDF in the default viewer\n"
      ],
      "metadata": {
        "id": "KgebHTBtZqve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21 Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree with max_depth=3\n",
        "dt_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_limited.fit(X_train, y_train)\n",
        "y_pred_limited = dt_limited.predict(X_test)\n",
        "acc_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Train fully grown Decision Tree (no max_depth)\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "acc_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy with max_depth=3: {acc_limited:.4f}\")\n",
        "print(f\"Accuracy with fully grown tree: {acc_full:.4f}\")\n"
      ],
      "metadata": {
        "id": "j7xLr5wcZydW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Decision Tree with min_samples_split=5\n",
        "dt_split5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "dt_split5.fit(X_train, y_train)\n",
        "y_pred_split5 = dt_split5.predict(X_test)\n",
        "acc_split5 = accuracy_score(y_test, y_pred_split5)\n",
        "\n",
        "# Default Decision Tree\n",
        "dt_default = DecisionTreeClassifier(random_state=42)\n",
        "dt_default.fit(X_train, y_train)\n",
        "y_pred_default = dt_default.predict(X_test)\n",
        "acc_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "# Print results\n",
        "print(f\"Accuracy with min_samples_split=5: {acc_split5:.4f}\")\n",
        "print(f\"Accuracy with default parameters: {acc_default:.4f}\")\n"
      ],
      "metadata": {
        "id": "MXiWJYZlaAuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train on unscaled data\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "acc_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train on scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Accuracy without scaling: {acc_unscaled:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "id": "R9__yHu0aJui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Wrap DecisionTreeClassifier with OneVsRestClassifier\n",
        "ovr_model = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
        "ovr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ovr_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"Accuracy using One-vs-Rest with Decision Tree: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "qs7BOnMgaRP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25 Write a Python program to train a Decision Tree Classifier and display the feature importance scores.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = clf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better display\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance Score': importances\n",
        "}).sort_values(by='Importance Score', ascending=False)\n",
        "\n",
        "# Display the feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "print(importance_df)\n"
      ],
      "metadata": {
        "id": "Xjm2U4bdaZhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26 Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor with max_depth=5\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "reg_limited.fit(X_train, y_train)\n",
        "y_pred_limited = reg_limited.predict(X_test)\n",
        "mse_limited = mean_squared_error(y_test, y_pred_limited)\n",
        "\n",
        "# Train unrestricted Decision Tree Regressor\n",
        "reg_full = DecisionTreeRegressor(random_state=42)\n",
        "reg_full.fit(X_train, y_train)\n",
        "y_pred_full = reg_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "\n",
        "# Compare MSEs\n",
        "print(f\"Mean Squared Error (max_depth=5): {mse_limited:.4f}\")\n",
        "print(f\"Mean Squared Error (unrestricted): {mse_full:.4f}\")\n"
      ],
      "metadata": {
        "id": "IYEY7eigagbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27 Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n",
        "\n",
        "# Train an initial Decision Tree to get effective alphas\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Train trees for each value of ccp_alpha\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Remove the last tree (usually trivial with one node)\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "# Compute train and test accuracy for each alpha\n",
        "train_scores = [accuracy_score(y_train, clf.predict(X_train)) for clf in clfs]\n",
        "test_scores = [accuracy_score(y_test, clf.predict(X_test)) for clf in clfs]\n",
        "\n",
        "# Plotting accuracy vs ccp_alpha\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, train_scores, marker='o', label=\"Train Accuracy\", drawstyle=\"steps-post\")\n",
        "plt.plot(ccp_alphas, test_scores, marker='o', label=\"Test Accuracy\", drawstyle=\"steps-post\")\n",
        "plt.xlabel(\"ccp_alpha (Pruning Parameter)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Effect of Cost Complexity Pruning on Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "t29ymbRpanJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28 Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate using precision, recall, and F1-score\n",
        "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)\n"
      ],
      "metadata": {
        "id": "VW1P61gHau6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29 Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn.\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix - Decision Tree Classifier')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hZFtIkxia7YL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#30 Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to search best parameters\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Output results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "s8SPw0p_bBGY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}